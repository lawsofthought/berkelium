{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimuli Selection for Experiments *Brisbane* and *Malmo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the British National Corpus\n",
    "\n",
    "The British National Corpus (BNC) is available from http://ota.ox.ac.uk/desc/2554 (with the reference guide available at http://www.natcorp.ox.ac.uk/docs/URG/index.html).\n",
    "\n",
    "Under the licence agreement, I am not at liberty to redistribute my copy of the BNC corpus here. However, to ensure reproducibility of what I describe below, the details of zip archive of the BNC that I obtained from the University of Oxford Text Archive (OTA), named `2554.zip`, are as follows:\n",
    "\n",
    "* Size: 539M\n",
    "* md5sum: 394a702072f2f3f62f467f8f911420e7\n",
    "* sha1sum: d0bbc6e29745bcddea42d2c57f5fa5e485002524 \n",
    "\n",
    "You can run the following linux commands to check the file size and checksum hashes of the BNC zip archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539M\t2554.zip\n",
      "394a702072f2f3f62f467f8f911420e7  2554.zip\n",
      "d0bbc6e29745bcddea42d2c57f5fa5e485002524  2554.zip\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "BNC_ZIP_FILE=2554.zip\n",
    "du -h $BNC_ZIP_FILE\n",
    "md5sum $BNC_ZIP_FILE\n",
    "sha1sum $BNC_ZIP_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the basis of the above info, you can guarantee if you have a bit-for-bit identical copy of the archive. Even if it is not bit-for-bit identical, the differences may still be of no practical consequence. In the file `2554.filelist.txt`, I provide a list of the contents of the archive. In the file, `2554.checksum.txt`, I provide the md5sum checksum of all the files. This will allow you to check to see if and where any differences exist between the files I am using and the BNC files you obtain from the OTA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the BNC xml file names\n",
    "\n",
    "For what follows below, I'm assuming the BNC corpus zip archive has been unzipped to `./bnc` and so the corpus xml files are below the `./bnc/2554/download/Texts/` directory. The following command will get the list of the xml files in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_filenames = utils.Corpus.get_corpus_filenames('./bnc/2554/download/Texts/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above command ran without problem, the number of `.xml` corpus files you have is {{ len(corpus_filenames) }}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up all BNC paragraphs\n",
    "\n",
    "The following commands will parse all the xml files in the corpus and read in the entire corpus as a list of paragraphs (using the BNC's own definition of what constitutes a paragraph). Each element of this list, will give the following\n",
    "\n",
    "* The xml filename which contains the paragraph\n",
    "* The top level division (div1) in the xml file that contains the paragraph\n",
    "* The order of the paragraph in the div1 division (i.e. is it the first, second, third, etc, paragraph)\n",
    "* How many paragraphs are in that div1 division\n",
    "* The paragraph text itself (i.e., raw text, no xml)\n",
    "* The paragraph text as a list of individual words (using BNC's own definition of what constitutes a word\n",
    "* The word length of the paragraph\n",
    "\n",
    "Note that this xml parsing and the data reading from disk is a slow process, and I so run it in parallel. I did so on a 16 core machine, and still it tool many hours. Once the parsing and reading in is down, a pickle file is created. Once this file is created, if `use_cached_data` is set to `True`, this pickle file is read in instead of running the parser again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_cached_data = False\n",
    "\n",
    "if not use_cached_data:\n",
    "    \n",
    "    from ipyparallel import Client\n",
    "\n",
    "    clients = Client()\n",
    "    clients.block = True\n",
    "    view = clients.load_balanced_view()\n",
    "\n",
    "    paragraphs = utils.get_all_paragraphs_parallel(view,\n",
    "                                                   corpus_filenames)\n",
    "    \n",
    "    paragraphs = sorted(paragraphs, \n",
    "                        key=lambda args: args['corpus_filename'])\n",
    "    \n",
    "    utils.dump(paragraphs, filename='paragraphs.pkl')\n",
    "\n",
    "    \n",
    "else:\n",
    "    \n",
    "    paragraphs = utils.load('paragraphs.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corpus vocabulary \n",
    "\n",
    "For our purposes, the vocabulary of the BNC is not set of all word types as defined by the BNC itself, i.e., those strings marked up with the `<w` ... `>` tag. This contains around 500K word types and includes many strings that we would not normally recognize as everyday words. \n",
    "\n",
    "To only use everyday words, we get the intersection of all the words in the BNC with those listed in the `2of4brif.txt` dictionary of around 60K English words. This vocabularly list is taken from the 12Dicts package of the SCOWL (And Friends) database of English words used for creating word list for spell checkers: http://wordlist.aspell.net/12dicts/. These files are in the public domain. \n",
    "\n",
    "We then remove all stopwords from this list. The stop word lists we used were `FoxStoplist.txt` and `SmartStoplist.txt` which were take from https://github.com/aneesha/RAKE.git (commit 22474be2ba9a88d78ea2f2efd8d1f8115af869e1) and are used here according to the MIT license for the repository.\n",
    "\n",
    "The three files `2of4brif.txt`, `FoxStoplist.txt`, `SmartStoplist.txt` are distributed with this notebook. You can check their file integrity with `md5sum` and this should give you:\n",
    "\n",
    "* 2of4brif.txt: 57fc602974b1ea0e8bb40f3b191ff100\n",
    "* FoxStoplist.txt: ffc5e787b820d4f4d552f03dc073423b\n",
    "* SmartStoplist.txt: 1430878775662c041a9ef7f48e491c8e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = utils.get_corpus_vocabulary(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get inverse document frequency\n",
    "\n",
    "For the purposes of extracting keywords from texts, we'll use a simple tfidf, i.e. term-frequency by inverse document frequency, definition of keywords. In other words, the keywords of a text are those words in our vocabulary, defined above, with the highest tfidf value. More precisely, for word $i$, its tfidf in a text is \n",
    "$$\n",
    "f_i \\times \\log(N/n_i) \n",
    "$$\n",
    "where $f_i$ is the frequency of occurrence of word $i$ in the text, $N$ is the total number of documents in the corpus, and $n_i$ is the total number of documents in the corpus where word $i$ occurs at least once. The latter term, i.e. $\\log(N/n_i)$ is the inverse document frequency. \n",
    "\n",
    "For present purposes, we defined a \"document\" as any paragraph with over 100 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = utils.get_inverse_document_frequency(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word association norms data\n",
    "\n",
    "One of the issues we will be addressing in this study is whether and to what extent memories of spoken and written language are predictable from word associations of the words in the text. As such, it is necessary to use a data set of word associations for the analysis. Also, we need to make sure that the text and word lists that we use in the memory experiment have sufficient numbers of example words in this data sets. \n",
    "\n",
    "The largest English language data-set currently in use was collected at http://www.smallworldofwords.com/en/, and here I am using an early release of this data provided by Simon De Deyne. \n",
    "\n",
    "The data is in a csv format and its checksum is\n",
    "* associations_en_05_01_2015.csv: 40df7669ab0751e2753a44540cd7c8a1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "md5sum associations_en_05_01_2015.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_association = utils.WordAssociations('associations_en_05_01_2015.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set has {{len(word_association.stimulus_words)}} unique stimulus words, with {{len(word_association.association_words)}} unique word associates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly sample paragraphs\n",
    "\n",
    "We want to randomly sample 50 paragraphs from the BNC. \n",
    "\n",
    "These paragraphs should all be around 150 +/- 10 words in length. They should have a specified density of words from our `2of4brif.txt` vocabulary list. This is to keep very strange and unusual texts that might be filled with e.g. jargon terms. Likewise, they should have a specified density of words that are stimulus words in the word association norms data set. \n",
    "\n",
    "We start, therefore, by simply filtering out paragraphs that met these criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paragraph_indices = utils.filter_paragraphs(paragraphs,\n",
    "                                            word_association,\n",
    "                                            minimum_length=140,\n",
    "                                            maximum_length=160,\n",
    "                                            density=(0.9,0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a set of {{len(paragraph_indices)}} paragraphs to choose from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will randomly sample 100 of these paragraphs. For each one, we will also create a list of its top twenty key words. This is to make the list of words for word list based memory tests. For recognition memory tests, we will need a set of words that are in the text or word list, and a set of words that are not in the text of word list. For the \"in\" or \"target\" list of words, this can simply be 10 of the 20 key words. For the \"out\" or \"lure\" list, we choose keywords from paragraphs that are adjacent, i.e. before and after, the selected paragraph. We will only choose relatively short neighbour paragraphs, i.e. between 50 and 150 words, each. These words must obviously not be in the paragraph. Nor do we want repetitions of morphologically related words. For example, we want only one of the following \"fox\", \"foxes\", \"foxing\", etc. Moreover, we don't even want any of these words to have morphological variants in the text. Finally, We also want all the key words in the text and so all the words in the word list to be in the set of stimulus words from the word association data set and we want all the target words and all the lure words to be in the list of word associates from the word association data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampled_paragraphs\\\n",
    "    = utils.ParagraphSampler.select_paragraphs(paragraphs,\n",
    "                                               paragraph_indices,\n",
    "                                               idf,\n",
    "                                               vocabulary,\n",
    "                                               word_association,\n",
    "                                               number_of_paragraphs=100,\n",
    "                                               list_lengths=(25,10),\n",
    "                                               neighbour_paragraph_length=(50,150),\n",
    "                                               association_word_density_threshold=1.0,\n",
    "                                               random_seed=12345)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, we will verify that all key words in the text and all the words in the word list are among the stimulus words, and all the target words and all the lure words are in the associate words, in the word association norm data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.verify_sample_paragraphs(sampled_paragraphs, word_association)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we select half the paragraphs to be used for word list based memory tests and half to be used text based memory tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.randomly_sample_as_texts_and_wordlists(sampled_paragraphs, random_seed=321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the sampled paragraphs to file. This will write the files as text and also as pickle, so don't provide a extension to the filename. That will be added automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.write_paragraphs_to_file(sampled_paragraphs, filename='sample-paragraphs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(utils.write_paragraphs_to_str(sampled_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
